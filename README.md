# Elastic Sparse Attention for Long-Sequence Modeling



<p align="center">
  <a href="https://doi.org/10.5281/zenodo.15871900">
    <img src="https://img.shields.io/badge/DOI-10.5281%2Fzenodo.15871900-blue.svg" alt="DOI"></a>
  <a href="https://discord.gg/zcJszfPrZs"><img src="https://img.shields.io/discord/857781525553741874" alt="Discord"></a>
  <a href="https://qm.qq.com/q/nZjcgtvfry"><img src="https://img.shields.io/badge/QQ-1044867291-0086F9" alt="Discord"></a>
</p>


This is the official implementation of [Elastic Sparse Attention for Long-Sequence Modeling](https://doi.org/10.5281/zenodo.15871900).

## License Agreement

All our open-weight models are licensed under Apache 2.0.

## Citation

If you find our work helpful, feel free to give us a cite.

```bibtex
@manual{wu_2025_15871900,
  title        = {Elastic Sparse Attention for Long-Sequence
                   Modeling
                  },
  author       = {Wu, Hecong},
  month        = jul,
  year         = 2025,
  doi          = {10.5281/zenodo.15871900},
  url          = {https://doi.org/10.5281/zenodo.15871900},
}
```
